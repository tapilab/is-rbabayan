{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tweepy \n",
    "import pickle\n",
    "import os\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TWEET_PATH = 'Tweets/'\n",
    "ACCESS_TOKEN = '3007366663-QU3WM6hrAXEAfelPzdCpv713LOB8D7LgtsuvZWL'\n",
    "ACCESS_SECRET = 'RzDTCkg3xoZfEmc3bGNXypponiq06ak9rZxiziXzx7nkO'\n",
    "CONSUMER_KEY = 'nlbCSYMdqtyKpANbLQOl6ITKZ'\n",
    "CONSUMER_SECRET = 'wY1CXDFLcN03H94BQo96KzKW47J2nfvoQBr4x5XL96POTV9Bbj'\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "api.wait_on_rate_limit = True\n",
    "api.wait_on_rate_limit_notify = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_tweets(user_id, api):\n",
    "\n",
    "\n",
    "    alltweets = []\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(user_id = user_id,count=200)\n",
    "\n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "\n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print \"getting tweets before %s\" % (oldest)\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(user_id = user_id, count=200, max_id=oldest)\n",
    "\n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "\n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "        print \"...%s tweets downloaded so far\" % (len(alltweets))\n",
    "    \n",
    "    return alltweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 201275158814670848\n",
      "...105 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "all_tweets = get_all_tweets(40428817,api)\n",
    "\n",
    "#for tweet in all_tweets:\n",
    "    #print tweet.coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('all_tweets.txt', 'w')\n",
    "pickle.dump(all_tweets, f)\n",
    "f.close()\n",
    "\n",
    "f = open('all_tweets.txt', 'r')\n",
    "all_tweets2 = pickle.load(f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3307149463, 151359968, 21360280, 4796152642, 6108862, 4698707863, 23233540, 2591284940, 1883372546, 21170191, 2535894696, 4254072565, 4192755922, 757365218, 1257839899, 4066538285, 4010212324, 3986161815, 3972756387, 3907999820, 271270022, 3877458434, 1250154102, 194730702, 95444678, 3297168955, 3335474890, 14227833, 1312991798, 3003284760, 3358099809, 3689063472, 1228607336, 3241707222, 131325228, 251895220, 3308251327, 1930455805, 3487529177, 3431526526, 236747363, 21699539, 3387531023, 3318498939, 2484331177, 3283267148, 3378439426, 3377078841, 3375860717, 471537646, 3132302816, 242000735, 3250737378, 3255544399, 3335620239, 3330054262, 2369220404, 2891321533, 3069826077, 3038389685, 252717796, 3144620312, 3196767276, 1866243193, 2464484016, 558838751, 791670, 37707571, 3140676266, 2876606781, 223548382, 3077211596, 3007366663, 1110846529, 2882625658, 46918361, 16611936, 2819259732, 571930726, 2787687136, 2828600952, 416109270, 2826747548, 589467785, 2800404157, 2785908954, 2318812388, 2783453490, 1902643855, 2780024642, 1602759349, 2725691708, 2719921982, 2717896135, 308638683, 2200186483, 1420559263, 2574480096, 2515695091, 26776603, 2242803776, 2514810294, 363242402, 2514673736, 833521344, 2519579221, 2414051160, 2474519773, 180083012, 2433832854, 2170113439, 2426629550, 1542965000, 2202075444, 26954169, 2368464894, 2327270828, 1486205131, 2330919194, 26560862, 2293155541, 345943433, 2211527166, 785710196, 964756267, 2182813212, 2154978493, 2159597790, 607595463, 1959038894, 356001827, 96004975, 355481804, 250190449, 269139470, 1712127769, 1694784222, 1523803256, 1530524983, 1615181888, 88611628, 40428817, 524217558, 536215243, 1531617572, 519525644, 1537054093, 1250742738, 1487518578, 1198400576, 5785682, 1568010072, 1535768448, 1507197852, 1267282981, 1137013590, 75778260, 123049717, 828761102, 794590184, 780388808, 767495744, 450025436, 539402325, 17729292, 553616593, 493936865, 211917520, 480510585, 357766472, 340745895, 334158135, 189125038]\n"
     ]
    }
   ],
   "source": [
    "def get_followers(filename, screen_name, api):\n",
    "    \n",
    "    uscis_followers = []\n",
    "    for page in tweepy.Cursor(api.followers_ids, screen_name ='SabaJamalian').pages():\n",
    "        uscis_followers.extend(page)\n",
    "        time.sleep(60)\n",
    "    \n",
    "    #write into a file\n",
    "    f = open(filename , 'w')\n",
    "    for id in uscis_followers:\n",
    "        f.write(str(id) + '\\n')\n",
    "    f.close()\n",
    "    \n",
    "    print uscis_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "173"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uscis_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prune_followers(uscis_followers, min_status_count, api):\n",
    "    \n",
    "    pruned = []\n",
    "    \n",
    "    for id in uscis_followers:\n",
    "        user = api.get_user(id)\n",
    "        \n",
    "        if not user.protected:\n",
    "            if user.geo_enabled:\n",
    "                if user.statuses_count > min_status_count:\n",
    "                    pruned.append(id)\n",
    "                    print id\n",
    "    \n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151359968\n",
      "21360280\n",
      "6108862\n",
      "23233540\n",
      "2535894696\n",
      "194730702\n",
      "95444678\n",
      "14227833\n",
      "251895220\n",
      "2464484016\n",
      "791670\n",
      "223548382\n",
      "46918361\n",
      "Rate limit reached. Sleeping for: 631\n",
      "416109270\n",
      "589467785\n",
      "26776603\n",
      "26560862\n",
      "96004975\n",
      "250190449\n",
      "269139470\n",
      "88611628\n",
      "40428817\n",
      "123049717\n",
      "493936865\n",
      "189125038\n"
     ]
    }
   ],
   "source": [
    "pruned_followers = prune_followers(uscis_followers, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_and_save_tweets_one_user(user_id, api):\n",
    "    all_tweets = get_all_tweets(user_id, api)\n",
    "    f = open (TWEET_PATH + str(user_id) , 'w')\n",
    "    pickle.dump(all_tweets, f)\n",
    "    f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_and_save_tweets_all_users(followers_pruned_id, api):\n",
    "    for id in followers_pruned_id:\n",
    "        get_and_save_tweets_one_user(id, api)\n",
    "        print 'tweets of ' , id , 'were saved!' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting tweets before 361546959447785472\n",
      "...187 tweets downloaded so far\n",
      "tweets of  151359968 were saved!\n",
      "getting tweets before 690401316326236159\n",
      "...386 tweets downloaded so far\n",
      "getting tweets before 684528610581979135\n",
      "...586 tweets downloaded so far\n",
      "getting tweets before 679937888797257727\n",
      "...786 tweets downloaded so far\n",
      "getting tweets before 676900095250604031\n",
      "...986 tweets downloaded so far\n",
      "getting tweets before 674053610343432191\n",
      "...1186 tweets downloaded so far\n",
      "getting tweets before 671592201877524479\n",
      "...1386 tweets downloaded so far\n",
      "getting tweets before 668145672907026432\n",
      "...1586 tweets downloaded so far\n",
      "getting tweets before 665804136600866816\n",
      "...1786 tweets downloaded so far\n",
      "getting tweets before 664173766168063999\n",
      "...1986 tweets downloaded so far\n",
      "getting tweets before 661897494754934784\n",
      "...2186 tweets downloaded so far\n",
      "getting tweets before 659609510550573055\n",
      "...2386 tweets downloaded so far\n",
      "getting tweets before 657678689673564159\n",
      "...2585 tweets downloaded so far\n",
      "getting tweets before 655852619915743231\n",
      "...2785 tweets downloaded so far\n",
      "getting tweets before 653737447981977599\n",
      "...2985 tweets downloaded so far\n",
      "getting tweets before 650777479452459007\n",
      "...3185 tweets downloaded so far\n",
      "getting tweets before 646043721432039423\n",
      "...3206 tweets downloaded so far\n",
      "getting tweets before 645062168354889727\n",
      "...3206 tweets downloaded so far\n",
      "tweets of  21360280 were saved!\n",
      "getting tweets before 694706352476852223\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 693646289242853375\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 692646302186917887\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 691838621100457983\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 691109509167710207\n",
      "...1196 tweets downloaded so far\n",
      "getting tweets before 690022005161201663\n",
      "...1396 tweets downloaded so far\n",
      "getting tweets before 687797658614939647\n",
      "...1596 tweets downloaded so far\n",
      "getting tweets before 687095114750103555\n",
      "...1796 tweets downloaded so far\n",
      "getting tweets before 686040658423988224\n",
      "...1996 tweets downloaded so far\n",
      "getting tweets before 685242298393833472\n",
      "...2196 tweets downloaded so far\n",
      "getting tweets before 684554258834927615\n",
      "...2396 tweets downloaded so far\n",
      "getting tweets before 682610549788561407\n",
      "...2596 tweets downloaded so far\n",
      "getting tweets before 678425403762061312\n",
      "...2796 tweets downloaded so far\n",
      "getting tweets before 675348730623627263\n",
      "...2996 tweets downloaded so far\n",
      "getting tweets before 670502762443055103\n",
      "...3196 tweets downloaded so far\n",
      "getting tweets before 665923728811913215\n",
      "...3229 tweets downloaded so far\n",
      "getting tweets before 665205334722547711\n",
      "...3229 tweets downloaded so far\n",
      "tweets of  6108862 were saved!\n",
      "getting tweets before 627454175652282367\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 606473078479196159\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 587603136874991615\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 562300136781787135\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 540185869312073727\n",
      "...1200 tweets downloaded so far\n",
      "getting tweets before 522794872680751104\n",
      "...1400 tweets downloaded so far\n",
      "getting tweets before 511245987021873151\n",
      "...1600 tweets downloaded so far\n",
      "getting tweets before 503326668640223231\n",
      "...1799 tweets downloaded so far\n",
      "getting tweets before 478149903856971775\n",
      "...1997 tweets downloaded so far\n",
      "getting tweets before 438003213552599039\n",
      "...2066 tweets downloaded so far\n",
      "getting tweets before 181389019375091712\n",
      "...2066 tweets downloaded so far\n",
      "tweets of  23233540 were saved!\n",
      "getting tweets before 671739802723745791\n",
      "...395 tweets downloaded so far\n",
      "getting tweets before 663048910865207295\n",
      "...591 tweets downloaded so far\n",
      "getting tweets before 642818665180626943\n",
      "...790 tweets downloaded so far\n",
      "getting tweets before 584073985035350015\n",
      "...990 tweets downloaded so far\n",
      "getting tweets before 501505238298525696\n",
      "...1079 tweets downloaded so far\n",
      "getting tweets before 472499779331567616\n",
      "...1079 tweets downloaded so far\n",
      "tweets of  2535894696 were saved!\n",
      "getting tweets before 654308002925821951\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 653943513441288193\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 653580984395370499\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 625693390034870276\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 548214208434028543\n",
      "...1199 tweets downloaded so far\n",
      "getting tweets before 522057728756580351\n",
      "...1399 tweets downloaded so far\n",
      "getting tweets before 521455667547484159\n",
      "...1599 tweets downloaded so far\n",
      "getting tweets before 228596540422365183\n",
      "...1629 tweets downloaded so far\n",
      "getting tweets before 117335166212505599\n",
      "...1629 tweets downloaded so far\n",
      "tweets of  194730702 were saved!\n",
      "getting tweets before 694946407036362752\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 694515530011508735\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 686608815660216319\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 672544873589903359\n",
      "...998 tweets downloaded so far\n",
      "getting tweets before 667048412517425152\n",
      "...1198 tweets downloaded so far\n",
      "getting tweets before 656574465648623616\n",
      "...1398 tweets downloaded so far\n",
      "getting tweets before 651835703396012031\n",
      "...1598 tweets downloaded so far\n",
      "getting tweets before 639569113334054911\n",
      "...1798 tweets downloaded so far\n",
      "getting tweets before 638553388100263935\n",
      "...1998 tweets downloaded so far\n",
      "getting tweets before 636187546200973315\n",
      "...2198 tweets downloaded so far\n",
      "getting tweets before 623146516006502399\n",
      "...2398 tweets downloaded so far\n",
      "getting tweets before 613708207295451135\n",
      "...2598 tweets downloaded so far\n",
      "getting tweets before 598944716516909055\n",
      "...2798 tweets downloaded so far\n",
      "getting tweets before 591205774757646336\n",
      "...2998 tweets downloaded so far\n",
      "getting tweets before 580100595064913919\n",
      "...3198 tweets downloaded so far\n",
      "getting tweets before 566271920099053567\n",
      "...3214 tweets downloaded so far\n",
      "getting tweets before 565174722850459647\n",
      "...3246 tweets downloaded so far\n",
      "getting tweets before 563141285176147967\n",
      "...3246 tweets downloaded so far\n",
      "tweets of  95444678 were saved!\n",
      "getting tweets before 571720372099264514\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 530468833136766975\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 473654994219302911\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 451473751243120639\n",
      "...997 tweets downloaded so far\n",
      "getting tweets before 412948043223531519\n",
      "...1195 tweets downloaded so far\n",
      "getting tweets before 386542777779380223\n",
      "...1393 tweets downloaded so far\n",
      "getting tweets before 362674784485642242\n",
      "...1588 tweets downloaded so far\n",
      "getting tweets before 315130524140187647\n",
      "...1785 tweets downloaded so far\n",
      "getting tweets before 288706410424975359\n",
      "...1977 tweets downloaded so far\n",
      "getting tweets before 254362793271255039\n",
      "...2174 tweets downloaded so far\n",
      "getting tweets before 224973200465997823\n",
      "...2374 tweets downloaded so far\n",
      "getting tweets before 200449165472448511\n",
      "...2572 tweets downloaded so far\n",
      "getting tweets before 177075895524270079\n",
      "...2768 tweets downloaded so far\n",
      "getting tweets before 137741775195488255\n",
      "...2966 tweets downloaded so far\n",
      "getting tweets before 62122484832538624\n",
      "...3163 tweets downloaded so far\n",
      "getting tweets before 6186092906680319\n",
      "...3180 tweets downloaded so far\n",
      "getting tweets before 4906172242993151\n",
      "...3180 tweets downloaded so far\n",
      "tweets of  14227833 were saved!\n",
      "getting tweets before 690668339618717696\n",
      "...388 tweets downloaded so far\n",
      "getting tweets before 685549255700828159\n",
      "...588 tweets downloaded so far\n",
      "getting tweets before 672201272297062399\n",
      "...788 tweets downloaded so far\n",
      "getting tweets before 659698697681268736\n",
      "...987 tweets downloaded so far\n",
      "getting tweets before 644862067850088448\n",
      "...1187 tweets downloaded so far\n",
      "getting tweets before 630651057408835583\n",
      "...1387 tweets downloaded so far\n",
      "getting tweets before 618918656198623231\n",
      "...1521 tweets downloaded so far\n",
      "getting tweets before 36985844535533567\n",
      "...1521 tweets downloaded so far\n",
      "tweets of  251895220 were saved!\n",
      "getting tweets before 648208753813143551\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 579417698607710207\n",
      "...600 tweets downloaded so far\n",
      "getting tweets before 545854049250467839\n",
      "...800 tweets downloaded so far\n",
      "getting tweets before 464346556716818431\n",
      "...1000 tweets downloaded so far\n",
      "getting tweets before 461057727574581247\n",
      "...1032 tweets downloaded so far\n",
      "getting tweets before 460025938643005439\n",
      "...1032 tweets downloaded so far\n",
      "tweets of  2464484016 were saved!\n",
      "getting tweets before 345639606508728320\n",
      "...399 tweets downloaded so far\n",
      "getting tweets before 230223716645220351\n",
      "...598 tweets downloaded so far\n",
      "getting tweets before 134423375945809919\n",
      "...798 tweets downloaded so far\n",
      "getting tweets before 25438625870\n",
      "...842 tweets downloaded so far\n",
      "getting tweets before 2160137578\n",
      "...842 tweets downloaded so far\n",
      "tweets of  791670 were saved!\n",
      "getting tweets before 431839631286366207\n",
      "...400 tweets downloaded so far\n",
      "getting tweets before 283896079282499583\n",
      "...538 tweets downloaded so far\n",
      "getting tweets before 35957770721165311\n",
      "...538 tweets downloaded so far\n",
      "tweets of  223548382 were saved!\n",
      "getting tweets before 396025638152192001\n",
      "...193 tweets downloaded so far\n",
      "tweets of  46918361 were saved!\n",
      "getting tweets before 457375894710288383\n",
      "...303 tweets downloaded so far\n",
      "getting tweets before 447683683856973824\n",
      "...303 tweets downloaded so far\n",
      "tweets of  416109270 were saved!\n",
      "getting tweets before 206079270752227327\n",
      "...209 tweets downloaded so far\n",
      "getting tweets before 206039278747254784\n",
      "...209 tweets downloaded so far\n",
      "tweets of  589467785 were saved!\n",
      "getting tweets before 16837328214\n",
      "...398 tweets downloaded so far\n",
      "getting tweets before 1445928663\n",
      "...417 tweets downloaded so far\n",
      "getting tweets before 1395078067\n",
      "...417 tweets downloaded so far\n",
      "tweets of  26776603 were saved!\n",
      "getting tweets before 474851494198403071\n",
      "...394 tweets downloaded so far\n",
      "getting tweets before 448877111231934463\n",
      "...592 tweets downloaded so far\n",
      "getting tweets before 441231791295180799\n",
      "...778 tweets downloaded so far\n",
      "getting tweets before 437072019470188544\n",
      "...960 tweets downloaded so far\n",
      "getting tweets before 431472175552667647\n",
      "...1144 tweets downloaded so far\n",
      "getting tweets before 418797884235325439\n",
      "...1162 tweets downloaded so far\n",
      "getting tweets before 417980702332694528\n",
      "...1162 tweets downloaded so far\n",
      "tweets of  26560862 were saved!\n",
      "getting tweets before 289246648880738303\n",
      "...269 tweets downloaded so far\n",
      "getting tweets before 7350455818\n",
      "...269 tweets downloaded so far\n",
      "tweets of  96004975 were saved!\n",
      "getting tweets before 35733211501961215\n",
      "...161 tweets downloaded so far\n",
      "tweets of  250190449 were saved!\n",
      "getting tweets before 340900947565871106\n",
      "...184 tweets downloaded so far\n",
      "tweets of  269139470 were saved!\n",
      "getting tweets before 449818925081980927\n",
      "...393 tweets downloaded so far\n",
      "getting tweets before 407251969384792063\n",
      "...589 tweets downloaded so far\n",
      "getting tweets before 399581962504908799\n",
      "...788 tweets downloaded so far\n",
      "getting tweets before 397759020498300927\n",
      "...984 tweets downloaded so far\n",
      "getting tweets before 392218231752392703\n",
      "...1176 tweets downloaded so far\n",
      "getting tweets before 382596463802327040\n",
      "...1372 tweets downloaded so far\n",
      "getting tweets before 374295598389657600\n",
      "...1568 tweets downloaded so far\n",
      "getting tweets before 370816870359642111\n",
      "...1761 tweets downloaded so far\n",
      "getting tweets before 367359430817353728\n",
      "...1955 tweets downloaded so far\n",
      "getting tweets before 366826239824101375\n",
      "...2153 tweets downloaded so far\n",
      "getting tweets before 362612782602006527\n",
      "...2350 tweets downloaded so far\n",
      "getting tweets before 349602547121651712\n",
      "...2534 tweets downloaded so far\n",
      "getting tweets before 317928722936250368\n",
      "...2725 tweets downloaded so far\n",
      "getting tweets before 281361958328991743\n",
      "...2924 tweets downloaded so far\n",
      "getting tweets before 244748849091080191\n",
      "...3118 tweets downloaded so far\n",
      "getting tweets before 239427367355969535\n",
      "...3121 tweets downloaded so far\n",
      "getting tweets before 239422596632961024\n",
      "...3121 tweets downloaded so far\n",
      "tweets of  88611628 were saved!\n",
      "getting tweets before 201275158814670848\n",
      "...105 tweets downloaded so far\n",
      "tweets of  40428817 were saved!\n",
      "getting tweets before 499419475159314431\n",
      "...393 tweets downloaded so far\n",
      "getting tweets before 449886415069794303\n",
      "...592 tweets downloaded so far\n",
      "getting tweets before 440124987056205824\n",
      "...788 tweets downloaded so far\n",
      "getting tweets before 404916574621949951\n",
      "...986 tweets downloaded so far\n",
      "getting tweets before 366744114844278786\n",
      "...1184 tweets downloaded so far\n",
      "getting tweets before 351991288536629247\n",
      "...1384 tweets downloaded so far\n",
      "getting tweets before 327508287908106239\n",
      "...1583 tweets downloaded so far\n",
      "getting tweets before 308958991805980673\n",
      "...1783 tweets downloaded so far\n",
      "getting tweets before 287658915808813055\n",
      "...1982 tweets downloaded so far\n",
      "getting tweets before 10984480247\n",
      "...2103 tweets downloaded so far\n",
      "getting tweets before 10484735586\n",
      "...2103 tweets downloaded so far\n",
      "tweets of  123049717 were saved!\n",
      "getting tweets before 379667688781778944\n",
      "...393 tweets downloaded so far\n",
      "getting tweets before 170113337596715007\n",
      "...393 tweets downloaded so far\n",
      "tweets of  493936865 were saved!\n",
      "getting tweets before 600520965840359423\n",
      "...397 tweets downloaded so far\n",
      "getting tweets before 560932386406690816\n",
      "...597 tweets downloaded so far\n",
      "getting tweets before 512470687978889215\n",
      "...791 tweets downloaded so far\n",
      "getting tweets before 153186440174518271\n",
      "...791 tweets downloaded so far\n",
      "tweets of  189125038 were saved!\n"
     ]
    }
   ],
   "source": [
    "get_and_save_tweets_all_users(pruned_followers, api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user_ids_Twetter_folder():\n",
    "    all_followers = []\n",
    "    for dirs, root, files in os.walk(TWEET_PATH):\n",
    "        for file in files:\n",
    "            if not file.startswith('.'):\n",
    "                all_followers.append(file) \n",
    "    return all_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123049717', '14227833', '151359968', '189125038', '194730702', '21360280', '223548382', '23233540', '2464484016', '250190449', '251895220', '2535894696', '26560862', '26776603', '269139470', '40428817', '416109270', '46918361', '493936865', '589467785', '6108862', '791670', '88611628', '95444678', '96004975']\n"
     ]
    }
   ],
   "source": [
    "all_followers = get_user_ids_Twetter_folder()\n",
    "print all_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_coordinates_to_country_name(coordinates):\n",
    "    str_coordinates = str(coordinates[0]) + ', '+ str(coordinates[1])\n",
    "    geolocator = Nominatim()\n",
    "    location = geolocator.reverse(str_coordinates)\n",
    "    address = location.address\n",
    "    if address is not None:\n",
    "        return address.split(',')[-1].strip().lower()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert_coordinates_to_country_name(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_immigrant_candidates(followers_ids):\n",
    "    \n",
    "    immigrants_candidates = {}\n",
    "    \n",
    "    for user_id in followers_ids:\n",
    "        \n",
    "        date_location_dic = {}\n",
    "        \n",
    "        file_name = TWEET_PATH + str(user_id)\n",
    "        f = open(file_name,'r')\n",
    "        tweets = pickle.load(f)\n",
    "        f.close()\n",
    "    \n",
    "        for tweet in tweets:\n",
    "            if tweet.coordinates is not None:\n",
    "                country = convert_coordinates_to_country_name(tweet.coordinates.values()[1])\n",
    "                date_location_dic[tweet.created_at] = country\n",
    "            \n",
    "        if len(set(date_location_dic.values())) > 1:\n",
    "            print user_id\n",
    "            immigrants_candidates[user_id] = date_location_dic\n",
    "        \n",
    "    return immigrants_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14227833\n"
     ]
    },
    {
     "ename": "GeocoderTimedOut",
     "evalue": "Service timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGeocoderTimedOut\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-b6450e360303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_immigrant_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_followers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-218-c238aa16a551>\u001b[0m in \u001b[0;36mfind_immigrant_candidates\u001b[0;34m(followers_ids)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinates\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mcountry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_coordinates_to_country_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                 \u001b[0mdate_location_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreated_at\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcountry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-196-d2cddafc60a2>\u001b[0m in \u001b[0;36mconvert_coordinates_to_country_name\u001b[0;34m(coordinates)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mstr_coordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgeolocator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlocation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeolocator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_coordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0maddress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maddress\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/geopy/geocoders/osm.pyc\u001b[0m in \u001b[0;36mreverse\u001b[0;34m(self, query, exactly_one, timeout, language)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s.reverse: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         return self._parse_json(\n\u001b[0;32m--> 248\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_geocoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexactly_one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         )\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda/lib/python2.7/site-packages/geopy/geocoders/base.pyc\u001b[0m in \u001b[0;36m_call_geocoder\u001b[0;34m(self, url, timeout, raw, requester, deserializer, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mURLError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"timed out\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mGeocoderTimedOut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Service timed out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;34m\"unreachable\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mGeocoderUnavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Service not available'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mGeocoderTimedOut\u001b[0m: Service timed out"
     ]
    }
   ],
   "source": [
    "dic = find_immigrant_candidates(all_followers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2014, 9, 6, 16, 21, 25), datetime.datetime(2014, 9, 17, 18, 50, 6), datetime.datetime(2014, 9, 4, 7, 5, 44), datetime.datetime(2014, 10, 21, 6, 48, 52), datetime.datetime(2014, 9, 6, 9, 19, 31), datetime.datetime(2014, 9, 6, 5, 25, 55), datetime.datetime(2014, 9, 4, 5, 56, 45), datetime.datetime(2014, 9, 8, 5, 41, 22), datetime.datetime(2014, 11, 12, 13, 18, 46), datetime.datetime(2014, 9, 7, 8, 52, 47), datetime.datetime(2014, 9, 8, 7, 51, 18), datetime.datetime(2014, 9, 4, 7, 32, 40), datetime.datetime(2014, 9, 4, 6, 6, 46), datetime.datetime(2014, 11, 13, 9, 34, 7), datetime.datetime(2014, 9, 7, 8, 55, 18), datetime.datetime(2014, 9, 7, 6, 48, 34), datetime.datetime(2014, 9, 4, 5, 57, 56), datetime.datetime(2014, 9, 4, 6, 14, 42), datetime.datetime(2015, 1, 8, 21, 45, 25), datetime.datetime(2014, 9, 6, 5, 27, 1), datetime.datetime(2014, 10, 21, 6, 45, 27), datetime.datetime(2014, 9, 12, 20, 50, 22), datetime.datetime(2014, 9, 6, 5, 27, 30), datetime.datetime(2014, 9, 4, 6, 0, 29), datetime.datetime(2014, 9, 4, 7, 25, 59), datetime.datetime(2014, 9, 9, 5, 13), datetime.datetime(2014, 11, 4, 6, 14, 37), datetime.datetime(2014, 11, 23, 15, 36, 42), datetime.datetime(2014, 12, 29, 16, 44, 13), datetime.datetime(2014, 9, 8, 4, 38, 48), datetime.datetime(2014, 9, 17, 19, 12, 4), datetime.datetime(2014, 9, 4, 7, 35, 36), datetime.datetime(2014, 9, 4, 5, 53, 47), datetime.datetime(2014, 9, 4, 6, 1, 52), datetime.datetime(2014, 9, 4, 6, 5, 31), datetime.datetime(2014, 11, 12, 13, 18, 14), datetime.datetime(2014, 9, 7, 8, 56, 13), datetime.datetime(2014, 9, 17, 18, 54, 56), datetime.datetime(2014, 9, 6, 5, 31, 8), datetime.datetime(2014, 11, 22, 14, 45, 37), datetime.datetime(2014, 9, 8, 5, 43, 49), datetime.datetime(2014, 9, 6, 5, 28, 32), datetime.datetime(2014, 11, 22, 14, 46, 6), datetime.datetime(2014, 9, 8, 7, 52, 3), datetime.datetime(2014, 10, 21, 6, 49, 33), datetime.datetime(2014, 10, 21, 6, 41, 11), datetime.datetime(2014, 9, 9, 15, 17, 8), datetime.datetime(2014, 9, 17, 19, 57, 59), datetime.datetime(2014, 9, 4, 15, 46, 32), datetime.datetime(2014, 9, 6, 5, 32, 26), datetime.datetime(2014, 12, 27, 21, 13, 51), datetime.datetime(2014, 9, 8, 4, 34, 41), datetime.datetime(2014, 9, 8, 5, 39, 26), datetime.datetime(2014, 9, 4, 15, 9, 13), datetime.datetime(2014, 9, 8, 13, 55, 12), datetime.datetime(2014, 9, 7, 9, 15, 6), datetime.datetime(2014, 9, 6, 16, 15, 29), datetime.datetime(2014, 9, 16, 12, 5, 42), datetime.datetime(2014, 9, 17, 20, 9, 36), datetime.datetime(2015, 4, 30, 7, 27, 23), datetime.datetime(2014, 10, 21, 6, 47, 19), datetime.datetime(2014, 9, 4, 15, 47, 53), datetime.datetime(2014, 9, 4, 5, 55, 16), datetime.datetime(2014, 9, 8, 4, 42, 45)]\n"
     ]
    }
   ],
   "source": [
    "print dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
